# Local LLM Service Configuration

# Model cache directory (mounted volume)
MODEL_CACHE_DIR=./models_cache

# Logging level
LOG_LEVEL=INFO

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
WS_PORT=8001

# Model Configuration
AUTO_LOAD_BEST_MODEL=true
DEFAULT_MODEL=Qwen-1.8B-Chat

# Hardware Configuration
FORCE_CPU=false
FORCE_GPU=false
MAX_MEMORY_GB=8

# Development
DEBUG=false
RELOAD=false

