# ğŸš€ Vibe LLM

A high-performance local LLM service with **real-time WebSocket streaming** and **hot-swappable models**. Built for speed, scalability, and that ChatGPT-like streaming experience.

## âœ¨ Features

- **ğŸ”¥ Real-time WebSocket Streaming**: Word-by-word generation like ChatGPT
- **âš¡ Hot-swappable Models**: Switch between models without restarting
- **ğŸ¯ Smart Length Control**: Respects word limits and stops appropriately
- **ğŸš€ GPU Acceleration**: CUDA support with automatic hardware detection
- **ğŸ“¡ REST + WebSocket APIs**: Multiple ways to interact
- **ğŸ”§ Production Ready**: Comprehensive error handling and logging

## ğŸƒâ€â™‚ï¸ Quick Start

### Prerequisites
- Python 3.10+
- NVIDIA GPU with CUDA (optional but recommended)
- 8GB+ RAM (16GB+ recommended for larger models)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/vibe-llm.git
   cd vibe-llm
   ```

2. **Set up virtual environment**
   ```bash
   python -m venv venv
   # Windows
   .\venv\Scripts\activate
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   # For faster downloads
   pip install hf_xet "huggingface_hub[hf_xet]"
   ```

4. **Start the service**
   ```bash
   python -m service.main
   ```

The service will start on `http://localhost:8000` with WebSocket at `ws://localhost:8000/ws`

## ğŸ® Usage

### WebSocket Streaming (Recommended)
```python
import asyncio
import websockets
import json

async def chat():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        # Send message
        message = {
            "message": "Tell me about quantum computing",
            "max_length": 100,
            "temperature": 0.7
        }
        await websocket.send(json.dumps(message))
        
        # Receive streaming response
        async for response in websocket:
            data = json.loads(response)
            if data["type"] == "chunk":
                print(data["content"], end="", flush=True)
            elif data["type"] == "complete":
                break

asyncio.run(chat())
```

### REST API
```bash
# Simple chat
curl -X POST "http://localhost:8000/chat/simple?message=Hello"

# Switch models
curl -X POST "http://localhost:8000/models/load" \
     -H "Content-Type: application/json" \
     -d '{"model_name": "Qwen2.5-7B-Instruct"}'

# List available models
curl "http://localhost:8000/models"
```

## ğŸ¤– Supported Models

| Model | Size | VRAM | Performance | Best For |
|-------|------|------|-------------|----------|
| **Qwen3-0.6B** | 1.2GB | ~2GB | âš¡âš¡âš¡ | Quick responses, testing |
| **Qwen2.5-3B** | 6GB | ~4GB | âš¡âš¡ | Balanced speed/quality |
| **Qwen2.5-7B** | 10GB | ~8GB | âš¡ | Best quality responses |

*Auto-detects your hardware and recommends optimal models*

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WebSocket     â”‚    â”‚   Model Manager  â”‚    â”‚   Qwen Models   â”‚
â”‚   Streaming     â”‚â—„â”€â”€â–ºâ”‚   (Hot-swap)     â”‚â—„â”€â”€â–ºâ”‚   (GPU/CPU)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API      â”‚    â”‚   Hardware       â”‚    â”‚   Token-by-     â”‚
â”‚   Endpoints     â”‚â—„â”€â”€â–ºâ”‚   Detection      â”‚â—„â”€â”€â–ºâ”‚   Token Gen     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

### Environment Variables
```bash
MODEL_CACHE_DIR=./models_cache  # Model storage location
CUDA_VISIBLE_DEVICES=0          # GPU selection
```

### Model Configuration
Models are auto-detected based on your hardware in `models/detector.py`:
- **12GB+ VRAM**: All models available
- **8GB+ VRAM**: Up to 7B models
- **4GB+ VRAM**: Up to 3B models
- **CPU only**: Smaller models

## ğŸš€ Performance

- **Real-time streaming**: ~50ms between tokens for smooth UX
- **GPU acceleration**: 10x faster than CPU-only
- **Smart caching**: Models stay loaded for instant switching
- **Optimized inference**: Token-by-token generation with proper sampling

## ğŸ§ª Testing

```bash
# Run unit tests
python -m pytest tests/

# Test WebSocket streaming
python simple_websocket_test.py

# Test model loading
python test_7b_model.py

# Load larger model
python load_7b.py
```

## ğŸ“š API Reference

### WebSocket Messages
```json
// Send
{
  "message": "Your prompt here",
  "max_length": 100,
  "temperature": 0.7,
  "top_p": 0.9,
  "model": "Qwen2.5-7B-Instruct"  // optional
}

// Receive
{"type": "start", "content": "Starting generation..."}
{"type": "chunk", "content": "Hello", "chunk_id": 1}
{"type": "complete", "content": "Generation complete", "total_chunks": 50}
{"type": "error", "content": "Error message"}
```

### REST Endpoints
- `GET /health` - Service health check
- `GET /models` - List available models  
- `POST /models/load` - Load/switch model
- `POST /chat/simple` - Simple chat endpoint
- `POST /chat` - Full chat with streaming
- `GET /system` - System information

## ğŸ› ï¸ Development

### Project Structure
```
vibe-llm/
â”œâ”€â”€ models/           # Model implementations
â”‚   â”œâ”€â”€ manager.py    # Model lifecycle management
â”‚   â”œâ”€â”€ qwen.py      # Qwen model wrapper
â”‚   â””â”€â”€ detector.py   # Hardware detection
â”œâ”€â”€ service/         # FastAPI service
â”‚   â”œâ”€â”€ main.py      # Main application
â”‚   â””â”€â”€ websocket.py # WebSocket handlers
â”œâ”€â”€ tests/           # Unit tests
â””â”€â”€ requirements.txt # Dependencies
```

### Contributing
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“ˆ Roadmap

- [ ] Support for more model families (Llama, Mistral, etc.)
- [ ] Model quantization (4-bit, 8-bit)
- [ ] Multi-GPU support
- [ ] Docker deployment
- [ ] Web UI interface
- [ ] Conversation memory/context
- [ ] Custom fine-tuned model support

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Qwen Team** for the excellent models
- **Hugging Face** for the transformers library
- **FastAPI** for the amazing web framework
- **The community** for testing and feedback

---

**Built with â¤ï¸ for the local LLM community**

*Get that real-time AI vibe without the cloud! ğŸŒŸ*